3208 343 1054
4605
Model(
  (backbone): CBraMod(
    (patch_embedding): PatchEmbedding(
      (positional_encoding): Sequential(
        (0): Conv2d(200, 200, kernel_size=(19, 7), stride=(1, 1), padding=(9, 3), groups=200)
      )
      (proj_in): Sequential(
        (0): Conv2d(1, 25, kernel_size=(1, 49), stride=(1, 25), padding=(0, 24))
        (1): GroupNorm(5, 25, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
        (3): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (4): GroupNorm(5, 25, eps=1e-05, affine=True)
        (5): GELU(approximate='none')
        (6): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (7): GroupNorm(5, 25, eps=1e-05, affine=True)
        (8): GELU(approximate='none')
      )
      (spectral_proj): Sequential(
        (0): Linear(in_features=101, out_features=200, bias=True)
        (1): Dropout(p=0.1, inplace=False)
      )
    )
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn_s): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (self_attn_t): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=200, out_features=800, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=800, out_features=200, bias=True)
          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (proj_out): Identity()
  )
  (classifier): Sequential(
    (0): Rearrange('b c s d -> b (c s d)')
    (1): Linear(in_features=19000, out_features=1000, bias=True)
    (2): ELU(alpha=1.0)
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=1000, out_features=200, bias=True)
    (5): ELU(alpha=1.0)
    (6): Dropout(p=0.1, inplace=False)
    (7): Linear(in_features=200, out_features=1, bias=True)
    (8): Rearrange('b 1 -> (b 1)')
  )
)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:13<00:00,  3.72it/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:02<00:00, 19.86it/s]
Epoch 1 : Training Loss: 0.62407, acc: 0.81037, pr_auc: 0.91722, roc_auc: 0.90410, LR: 0.00030, Time elapsed 0.27 mins
[[1395  123]
 [ 504 1186]]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 19.81it/s]
/home/aiotlab/miniconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:2776: UserWarning: y_pred contains classes not in y_true
  warnings.warn("y_pred contains classes not in y_true")
/home/aiotlab/miniconda3/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.
  warnings.warn(
/home/aiotlab/miniconda3/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.
  warnings.warn(
val Evaluation: acc: 0.97085, pr_auc: 0.50000, roc_auc: nan
[[333  10]
 [  0   0]]
roc_auc increasing....saving weights !!
Val Evaluation: acc: 0.97085, pr_auc: 0.50000, roc_auc: nan
 25%|███████████████████████████                                                                               | 13/51 [00:02<00:06,  5.68it/s]
Traceback (most recent call last):
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/finetune_kfold_pearl.py", line 108, in <module>
    main()
    ~~~~^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/finetune_kfold_pearl.py", line 84, in main
    acc_, pr_auc_, roc_auc_ = t.train_for_binaryclass()
                              ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/finetune_trainer_kfold.py", line 89, in train_for_binaryclass
    loss.backward()
    ~~~~~~~~~~~~~^^
  File "/home/aiotlab/miniconda3/lib/python3.13/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/aiotlab/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/aiotlab/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
KeyboardInterrupt
