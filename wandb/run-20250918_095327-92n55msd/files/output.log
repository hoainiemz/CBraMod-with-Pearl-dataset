503 112 137
752
Model(
  (backbone): CBraMod(
    (patch_embedding): PatchEmbedding(
      (positional_encoding): Sequential(
        (0): Conv2d(200, 200, kernel_size=(19, 7), stride=(1, 1), padding=(9, 3), groups=200)
      )
      (proj_in): Sequential(
        (0): Conv2d(1, 25, kernel_size=(1, 49), stride=(1, 25), padding=(0, 24))
        (1): GroupNorm(5, 25, eps=1e-05, affine=True)
        (2): GELU(approximate='none')
        (3): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (4): GroupNorm(5, 25, eps=1e-05, affine=True)
        (5): GELU(approximate='none')
        (6): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
        (7): GroupNorm(5, 25, eps=1e-05, affine=True)
        (8): GELU(approximate='none')
      )
      (spectral_proj): Sequential(
        (0): Linear(in_features=101, out_features=200, bias=True)
        (1): Dropout(p=0.1, inplace=False)
      )
    )
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-11): 12 x TransformerEncoderLayer(
          (self_attn_s): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (self_attn_t): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=200, out_features=800, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=800, out_features=200, bias=True)
          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (proj_out): Identity()
  )
  (meta_backbone): Sequential(
    (0): Linear(in_features=28, out_features=100, bias=True)
    (1): ELU(alpha=1.0)
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=100, out_features=200, bias=True)
  )
  (arranger): Rearrange('b c s d -> b (c s) d')
  (attention): ParamAttention(
    (Wq): Linear(in_features=200, out_features=200, bias=False)
    (Wk): Linear(in_features=200, out_features=200, bias=False)
    (Wv): Linear(in_features=200, out_features=200, bias=False)
    (drop): Dropout(p=0.1, inplace=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=200, out_features=1, bias=True)
    (1): Rearrange('b 1 -> (b 1)')
  )
)
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:07<00:00,  4.55it/s]
Epoch 1 : Training Loss: 0.64614, LR: 0.00030, Time elapsed 0.12 mins
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 15.17it/s]
val Evaluation: acc: 0.42679, pr_auc: 0.38458, roc_auc: 0.33078
[[30 27]
 [37 18]]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 16.36it/s]
test Evaluation: acc: 0.60084, pr_auc: 0.44835, roc_auc: 0.58283
[[39 37]
 [19 42]]
roc_auc increasing....saving weights !!
Val Evaluation: acc: 0.42679, pr_auc: 0.38458, roc_auc: 0.33078
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  5.84it/s]
Epoch 2 : Training Loss: 0.33014, LR: 0.00029, Time elapsed 0.09 mins
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 17.04it/s]
val Evaluation: acc: 0.48820, pr_auc: 0.36910, roc_auc: 0.23955
[[37 20]
 [37 18]]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 18.58it/s]
test Evaluation: acc: 0.60084, pr_auc: 0.39346, roc_auc: 0.47606
[[39 37]
 [19 42]]
 38%|███████████████████████████████████████▊                                                                  | 12/32 [00:02<00:03,  5.90it/s]
Traceback (most recent call last):
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/finetune_kfold_pearl.py", line 112, in <module>
    main()
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/finetune_kfold_pearl.py", line 86, in main
    acc_, pr_auc_, roc_auc_ = t.train_for_binaryclass()
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/finetune_trainer_kfold.py", line 85, in train_for_binaryclass
    pred = self.model(x)
           ^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/models/model_for_pearl.py", line 142, in multi_attend_forward
    f1 = fork(self.eeg_backbone_attend, x[0])
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/jit/_async.py", line 99, in fork
    return torch._C.fork(func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/models/model_for_pearl.py", line 151, in eeg_backbone_attend
    return self.arranger(self.backbone(x))
                         ^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/models/cbramod.py", line 29, in forward
    feats = self.encoder(patch_emb)
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/models/criss_cross_transformer.py", line 29, in forward
    output = mod(output, src_mask=mask)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/models/criss_cross_transformer.py", line 90, in forward
    x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/namth/EEGFoundationModel/CBraMod/models/criss_cross_transformer.py", line 102, in _sa_block
    xs = self.self_attn_s(xs, xs, xs,
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1380, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/disk1/aiotlab/envs/cbramod/lib/python3.12/site-packages/torch/nn/functional.py", line 6484, in multi_head_attention_forward
    attn_output = scaled_dot_product_attention(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
